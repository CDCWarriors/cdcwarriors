{"nbformat_minor": 2, "cells": [{"source": "1) Create Google Cloud Project, Google CLoud STorage, Dataproc Cluster\nBlog - https://medium.com/google-cloud/apache-spark-and-jupyter-notebooks-made-easy-with-dataproc-component-gateway-fa91d48d6a5a\n\nCode - https://cloud.google.com/solutions/monte-carlo-methods-with-hadoop-spark\n\n2) Learn spark dataframe\nhttps://docs.databricks.com/getting-started/spark/dataframes.html#apache-spark-dataframes-notebook\n\nFree community Edition to learn spark - community.cloud.databricks.com/login.html\n\nReferences: https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n\npyspark - https://medium.com/@ravishankar_22148/billions-of-rows-milliseconds-of-time-pyspark-starter-guide-c1f984023bf2\n\n3) Tweet-ids stored manually in Google Storage Bucket\n- https://github.com/echen102/COVID-19-TweetIDs\n\n4) Python code to craete a Dateframe from all tweet ids from Google Storage bucket\n- Learn dataframe\n\n5) Use twart to hydrate tweets inside mapByPartition\n- https://github.com/echen102/COVID-19-TweetIDs/blob/master/hydrate.py\n- mapByPartition Example: https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8\n\n6) Develop python code and submit it to Dataproc\n\nExample: https://github.com/GoogleCloudPlatform/cloud-dataproc/tree/master/codelabs/spark-nlp/topic_model.py\n\nhttps://codelabs.developers.google.com/codelabs/spark-nlp/#7\n\nFor example:\ngcloud dataproc jobs submit pyspark --cluster ${CLUSTER_NAME}\\\n    --properties=spark.jars.packages=JohnSnowLabs:spark-nlp:2.0.8 \\\n    --driver-log-levels root=FATAL \\\n    topic_model.py \\\n    -- ${BUCKET_NAME}", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n  .appName('Tweet Analysis App') \\\n  .enableHiveSupport()  \\\n  .getOrCreate()", "outputs": [], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)", "outputs": [], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "from google.cloud import storage\n\ngcs_client = storage.Client()\nbucket = gcs_client.bucket('tweet-ids')\n\n#list(bucket.list_blobs(prefix='2020-01/'))", "outputs": [], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "df2 = spark \\\n  .read \\\n  .option ( \"inferSchema\" , \"true\" ) \\\n  .csv ( \"gs://tweet-ids/2020-01/*\" )", "outputs": [], "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "df2.printSchema", "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "<bound method DataFrame.printSchema of DataFrame[_c0: bigint]>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "cnt = df2.count()\ncnt", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "25014996"}, "metadata": {}}], "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "df2.rdd.getNumPartitions()", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "22"}, "metadata": {}}], "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "df3 = df2.repartition(200)", "outputs": [], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "df3.rdd.getNumPartitions()", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "200"}, "metadata": {}}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def hydrateTweets():\n    #use twart api code\n    print(hydrateTweet)\n\n#df3.mapPartitions(shouldHydrateTweets)\n#save the output of each partition\n#df3.partitionBy().options().save(gs://) \n#parquet ", "outputs": [], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "iterator = df3.mapPartitions(\n      (it: Iterator[Row]) =>\n        it.toList\n          .map(row => {\n            println(row)\n            #use twart api to get the tweets\n            dataframeRow = (row.toString)\n            dataframeRow\n          })\n          .iterator\n    )\n    .toLocalIterator", "outputs": [{"ename": "SyntaxError", "evalue": "invalid syntax (<ipython-input-17-2907a8fe99ca>, line 2)", "traceback": ["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-2907a8fe99ca>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    (it: Iterator[Row]) =>\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"], "output_type": "error"}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import sys\nimport getpass\n\n!{sys.executable} -m pip install numpy\nsys.path\nnumpy.__path__\n\nprint('This job is running as \"{}\".'.format(getpass.getuser()))\nprint(sys.executable, sys.version_info)", "outputs": [], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "#!conda list | grep pyspark\n#!conda config --append channels conda-forge\n#!conda update -n base conda\n#!conda install --yes --prefix {sys.prefix} spark-nlp\n#!conda search spark-nlp\n#import pyspark\n#import imp\n#pyspark.__path__\n#print(\"Find ModuleInfo :\",imp.find_module('pyspark'))\n#pyspark.__version__\n#!conda install -c johnsnowlabs spark-nlp=2.3.4\n#!conda install -c johnsnowlabs spark-nlp\n#!conda remove spark-nlp\n!pip install spark-nlp==2.2.1\n#!conda list | grep spark-nlp\n#import imp\n#print(imp.find_module('sparknlp'))\n#!conda remove spark-nlp\n#!conda install -c johnsnowlabs spark-nlp=2.2.1", "outputs": [{"output_type": "stream", "name": "stdout", "text": "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\nCollecting spark-nlp==2.2.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/89/16492f3c61c6cfc9b41344de7c1c875f171c33cba1b8b9546dd0c4d77ff6/spark_nlp-2.2.1-py2.py3-none-any.whl (64kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.2MB/s \n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-2.2.1\n"}], "metadata": {}}, {"source": "Data Analysis", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "import sparknlp\nfrom sparknlp.annotator import Lemmatizer, Stemmer, Tokenizer, Normalizer\nfrom sparknlp.base import DocumentAssembler, Finisher\n# A Spark Session is how we interact with Spark SQL to create Dataframes\nfrom pyspark.sql import SparkSession\n# These allow us to create a schema for our data\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n\n# Spark Pipelines allow us to sequentially add components such as transformers \nfrom pyspark.ml import Pipeline\n\n# These are components we will incorporate into our pipeline.\nfrom pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF\n\n# LDA is our model of choice for topic modeling\nfrom pyspark.ml.clustering import LDA\n\n# Some transformers require the usage of other Spark ML functions. We import them here\nfrom pyspark.sql.functions import col, lit, concat\n\n# This will help catch some PySpark errors\nfrom pyspark.sql.utils import AnalysisException", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.15", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}