{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigDataTweetApp.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMism2HWnLnpAhtWTfsEOuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaniska/covid-19-hackathon/blob/master/social-network-mining/BigDataTweetAnalysisApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X4CF_KZ8PgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/jupyter\n",
        "\n",
        "Colab Notebook\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb\n",
        "\n",
        "Jupyter Notebook using pandas pickle\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/clinical_text_classification/2.Adverse_Effect_Classification.ipynb\n",
        "\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/Spark%20NLP%20Healthcare%20Training%20-%20April%202020.pdf\n",
        "\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/clinical_text_classification/2.Adverse_Effect_Classification.ipynb\n",
        "\n",
        "Spark-NLP Databrics\n",
        "https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html#Getting%20Started.html\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/databricks\n",
        "\n",
        "https://nlp.johnsnowlabs.com/docs/en/install#python\n",
        "\n",
        "#Natural Language Processing with PySpark and Spark-NLP\n",
        "https://towardsdatascience.com/natural-language-processing-with-pyspark-and-spark-nlp-b5b29f8faba\n",
        "\n",
        "#More code to run spark-nlp in clouddataproc\n",
        "https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/codelabs/spark-nlp/topic_model.py\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmNDxv7zSl4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! update-alternatives --config java\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-FCHQCz6I15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "! pip install --ignore-installed spark-nlp==2.4.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OFFVwYc5M05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\")\n",
        "sparknlp.version()\n",
        "print(\"Apache Spark version\")\n",
        "spark.version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cLepNKGcc-M",
        "colab_type": "text"
      },
      "source": [
        "Alternate way of initializing spark with sparknlp\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ner\")\\\n",
        "    .master(\"local[4]\")\\\n",
        "    .config(\"spark.driver.memory\",\"8G\")\\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4\")\\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PMblszm5aNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGl72KEp5a7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline1 = PretrainedPipeline('recognize_entities_dl', 'en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_15FH7bLCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotation = pipeline1.annotate(\"covid19 PPE available Kaiser Permamnente Medical Fremont\")\n",
        "print(annotation['ner'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1NarkxtbTeZ",
        "colab_type": "text"
      },
      "source": [
        "Tutorial - https://nlp.johnsnowlabs.com/docs/en/pipelines#recognize_entities_dl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbo3QZDSRaIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import *\n",
        "\n",
        "SampleData = Row(\"id\", \"text\")\n",
        "sample1 = SampleData(1, \"Google has announced the release of a beta version of the popular TensorFlow machine learning library\")\n",
        "sample2 = SampleData(2, \"Donald John Trump (born June 14, 1946) is the 45th and current president of the United States\")\n",
        "sample3 = SampleData(3,\"covid19 PPE available Kaiser Permamnente Medical Fremont\")\n",
        "samplesSeq = [sample1, sample2, sample3]\n",
        "sampleDF = spark.createDataFrame(samplesSeq)\n",
        "\n",
        "#SparkSession.createDataFrame(rows)\n",
        "#pipeline1.annotate(...)\n",
        "annotation = pipeline1.transform(sampleDF)\n",
        "#print(annotation['ner'])\n",
        "annotation.select(\"entities.result\").show(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9QYe52fTvLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline2 = PretrainedPipeline('analyze_sentiment', 'en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj3BcS07Tzbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipeline2.annotate('I am hopeful that we shall win over covid19 ')\n",
        "print(result['sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9VmU2te71RL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spark-nlp components. Each one is incorporated into our pipeline.\n",
        "from sparknlp.annotator import Lemmatizer, Stemmer, Tokenizer, Normalizer\n",
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "\n",
        "# A Spark Session is how we interact with Spark SQL to create Dataframes\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# These allow us to create a schema for our data\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "\n",
        "# Spark Pipelines allow us to sequentially add components such as transformers \n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# These are components we will incorporate into our pipeline.\n",
        "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF\n",
        "\n",
        "# LDA is our model of choice for topic modeling\n",
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "# Some transformers require the usage of other Spark ML functions. We import them here\n",
        "from pyspark.sql.functions import col, lit, concat\n",
        "\n",
        "# This will help catch some PySpark errors\n",
        "from pyspark.sql.utils import AnalysisException"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFgvI9q073N2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, we begin assembling our pipeline. Each component here is used to some transformation to the data.\n",
        "# The Document Assembler takes the raw text data and convert it into a format that can\n",
        "# be tokenized. It becomes one of spark-nlp native object types, the \"Document\".\n",
        "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "# The Tokenizer takes data that is of the \"Document\" type and tokenizes it. \n",
        "# While slightly more involved than this, this is effectively taking a string and splitting\n",
        "# it along ths spaces, so each word is its own string. The data then becomes the \n",
        "# spark-nlp native type \"Token\".\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "# The Normalizer will group words together based on similar semantic meaning. \n",
        "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalizer\")\n",
        "\n",
        "# The Stemmer takes objects of class \"Token\" and converts the words into their\n",
        "# root meaning. For instance, the words \"cars\", \"cars'\" and \"car's\" would all be replaced\n",
        "# with the word \"car\".\n",
        "stemmer = Stemmer().setInputCols([\"normalizer\"]).setOutputCol(\"stem\")\n",
        "\n",
        "# The Finisher signals to spark-nlp allows us to access the data outside of spark-nlp\n",
        "# components. For instance, we can now feed the data into components from Spark MLlib. \n",
        "finisher = Finisher().setInputCols([\"stem\"]).setOutputCols([\"to_spark\"]).setValueSplitSymbol(\" \")\n",
        "\n",
        "# Stopwords are common words that generally don't add much detail to the meaning\n",
        "# of a body of text. In English, these are mostly \"articles\" such as the words \"the\"\n",
        "# and \"of\".\n",
        "stopword_remover = StopWordsRemover(inputCol=\"to_spark\", outputCol=\"filtered\")\n",
        "\n",
        "# Here we implement TF-IDF as an input to our LDA model. CountVectorizer (TF) keeps track\n",
        "# of the vocabulary that's being created so we can map our topics back to their\n",
        "# corresponding words.\n",
        "# TF (term frequency) creates a matrix that counts how many times each word in the\n",
        "# vocabulary appears in each body of text. This then gives each word a weight based\n",
        "# on its frequency.\n",
        "tf = CountVectorizer(inputCol=\"filtered\", outputCol=\"raw_features\")\n",
        "\n",
        "# Here we implement the IDF portion. IDF (Inverse document frequency) reduces \n",
        "# the weights of commonly-appearing words. \n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# LDA creates a statistical representation of how frequently words appear \n",
        "# together in order to create \"topics\" or groups of commonly appearing words.\n",
        "lda = LDA(k=10, maxIter=10)\n",
        "\n",
        "# We add all of the transformers into a Pipeline object. Each transformer\n",
        "# will execute in the ordered provided to the \"stages\" parameter\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document_assembler,\n",
        "        tokenizer,\n",
        "        normalizer,        \n",
        "        stemmer,\n",
        "        finisher,\n",
        "        stopword_remover,\n",
        "        tf,\n",
        "        idf,\n",
        "        lda\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
